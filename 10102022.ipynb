{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this line if using Colab.\n",
    "!git clone https://github.com/pearl-yu/twitch_project\n",
    "# %cd foster_2022fall/Module2_Supervised/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76edb381",
   "metadata": {},
   "source": [
    "# ignore this cell for now\n",
    "from endpoint_call import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f9c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # get the content from the API by using the get() method. json() method converts the API response to JSON format for easy handling.\n",
    "import json   # to work with the returned json content from API.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec729766",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pearl's credentials\n",
    "client_id = 'tamrylbvlu3wub4r8c5fddwsou246x'\n",
    "client_secret= 'y4enh38kpiwkh7j00nqvv185ttsedw'\n",
    "\n",
    "# Defining the URL to which we will make the request\n",
    "url = 'https://id.twitch.tv/oauth2/token'\n",
    "# Defining the parameters to be passed to the request\n",
    "body = {\n",
    "  'client_id'     : client_id,\n",
    "  'client_secret' : client_secret,\n",
    "  'grant_type'    : 'client_credentials'\n",
    "}\n",
    "\n",
    "access_code = requests.post(url, params=body) # Making a POST request to the URL to retrieve the access token\n",
    "access_token = json.loads(access_code.text) #access token response is a JSON-encoded app access token\n",
    "\n",
    "access_token = access_token['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f59ea6",
   "metadata": {},
   "source": [
    "With the sample:\n",
    "- Pull user ids by calling the https://dev.twitch.tv/docs/api/reference#get-users endpoint, specifying the login parameter.\n",
    "- Pull the video ids following this: https://github.com/btjmga/twitch-getallvods  (I didn't have the code to pull ALL the vids)\n",
    "- Pull the chatfiles of these videos. (using the downloader https://github.com/lay295/TwitchDownloader is easy. Check  https://github.com/Chaparro/rechat-dl and https://github.com/malciin/twitch-comments-downloader for code.)\n",
    "- Construct variables from the chatfile.\n",
    "- Call what other stuff? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc61477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_info(url = 'https://api.twitch.tv/helix/users?id=', key_series = df['Channel'], n = 5):\n",
    "    \n",
    "    headers = {\n",
    "            'Authorization' : 'Bearer '+str(access_token),\n",
    "            'Client-Id' : client_id\n",
    "        }\n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    for i in range(0,5) :\n",
    "        temp_response = requests.get(url+str(key_series[i]), headers=headers)\n",
    "        # Load the JSON\n",
    "        temp_response_json = json.loads(temp_response.text)\n",
    "        if list(temp_response_json.keys())[0] == 'error':\n",
    "            continue\n",
    "        else:\n",
    "            temp_data = temp_response_json['data']\n",
    "            temp_df_temp = pd.DataFrame.from_dict(json_normalize(temp_data), orient='columns')\n",
    "        \n",
    "            frames = [temp_df, temp_df_temp]\n",
    "            temp_df = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    print(temp_df.shape)\n",
    "    print(temp_df.columns)\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1cd2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Watch time (mins)</th>\n",
       "      <th>Stream time (mins)</th>\n",
       "      <th>Peak viewers</th>\n",
       "      <th>Average viewers</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Followers gained</th>\n",
       "      <th>Partnered</th>\n",
       "      <th>Mature</th>\n",
       "      <th>Language</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://static-cdn.jtvnw.net/jtv_user_pictures...</td>\n",
       "      <td>KaiCenat</td>\n",
       "      <td>750515550</td>\n",
       "      <td>13845</td>\n",
       "      <td>174509</td>\n",
       "      <td>54208</td>\n",
       "      <td>2402017</td>\n",
       "      <td>346668</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>English</td>\n",
       "      <td>KaiCenat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://static-cdn.jtvnw.net/jtv_user_pictures...</td>\n",
       "      <td>ElSpreen</td>\n",
       "      <td>318862545</td>\n",
       "      <td>5505</td>\n",
       "      <td>173221</td>\n",
       "      <td>57922</td>\n",
       "      <td>5618251</td>\n",
       "      <td>230563</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>ElSpreen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://static-cdn.jtvnw.net/jtv_user_pictures...</td>\n",
       "      <td>PaulinhoLOKObr</td>\n",
       "      <td>412465830</td>\n",
       "      <td>6075</td>\n",
       "      <td>141526</td>\n",
       "      <td>67895</td>\n",
       "      <td>1235167</td>\n",
       "      <td>222035</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>PaulinhoLOKObr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://static-cdn.jtvnw.net/jtv_user_pictures...</td>\n",
       "      <td>WestCOL</td>\n",
       "      <td>172343475</td>\n",
       "      <td>5460</td>\n",
       "      <td>145907</td>\n",
       "      <td>31564</td>\n",
       "      <td>863854</td>\n",
       "      <td>212347</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>WestCOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://static-cdn.jtvnw.net/jtv_user_pictures...</td>\n",
       "      <td>SypherPK</td>\n",
       "      <td>134891265</td>\n",
       "      <td>14730</td>\n",
       "      <td>151092</td>\n",
       "      <td>9157</td>\n",
       "      <td>6213265</td>\n",
       "      <td>205617</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>English</td>\n",
       "      <td>SypherPK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Unnamed: 1  \\\n",
       "0           1  https://static-cdn.jtvnw.net/jtv_user_pictures...   \n",
       "1           2  https://static-cdn.jtvnw.net/jtv_user_pictures...   \n",
       "2           3  https://static-cdn.jtvnw.net/jtv_user_pictures...   \n",
       "3           4  https://static-cdn.jtvnw.net/jtv_user_pictures...   \n",
       "4           5  https://static-cdn.jtvnw.net/jtv_user_pictures...   \n",
       "\n",
       "          Channel  Watch time (mins)  Stream time (mins)  Peak viewers  \\\n",
       "0        KaiCenat          750515550               13845        174509   \n",
       "1        ElSpreen          318862545                5505        173221   \n",
       "2  PaulinhoLOKObr          412465830                6075        141526   \n",
       "3         WestCOL          172343475                5460        145907   \n",
       "4        SypherPK          134891265               14730        151092   \n",
       "\n",
       "   Average viewers  Followers  Followers gained  Partnered  Mature  \\\n",
       "0            54208    2402017            346668       True   False   \n",
       "1            57922    5618251            230563       True   False   \n",
       "2            67895    1235167            222035       True   False   \n",
       "3            31564     863854            212347       True   False   \n",
       "4             9157    6213265            205617       True   False   \n",
       "\n",
       "     Language     Unnamed: 12  \n",
       "0     English        KaiCenat  \n",
       "1     Spanish        ElSpreen  \n",
       "2  Portuguese  PaulinhoLOKObr  \n",
       "3     Spanish         WestCOL  \n",
       "4     English        SypherPK  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/sample_by_growing_sullygnome/Fastest growing Twitch streamers, past 30 days - SullyGnome.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe05125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-c4da90f1ac8d>:17: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  temp_df_temp = pd.DataFrame.from_dict(json_normalize(temp_data), orient='columns')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "Index(['id', 'login', 'display_name', 'type', 'broadcaster_type',\n",
      "       'description', 'profile_image_url', 'offline_image_url', 'view_count',\n",
      "       'created_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## call the get_users end points to retrieve user id\n",
    "user_df = api_info(url = 'https://api.twitch.tv/helix/users?login=',\n",
    "         key_series = df['Channel'], \n",
    "         n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de9286e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-c4da90f1ac8d>:17: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  temp_df_temp = pd.DataFrame.from_dict(json_normalize(temp_data), orient='columns')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 17)\n",
      "Index(['id', 'stream_id', 'user_id', 'user_login', 'user_name', 'title',\n",
      "       'description', 'created_at', 'published_at', 'url', 'thumbnail_url',\n",
      "       'viewable', 'view_count', 'language', 'type', 'duration',\n",
      "       'muted_segments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## call the get_videos end points to retrieve all video ids of the users\n",
    "video_df = api_info(url = 'https://api.twitch.tv/helix/videos?user_id=',\n",
    "         key_series = user_df['id'], \n",
    "         n = 5)\n",
    "\n",
    "video_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510de2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve chat files of these videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f4a27",
   "metadata": {},
   "source": [
    "Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse chat file\n",
    "def get_chat_dataframe(file):\n",
    "    data = []\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        for line in lines:\n",
    "            #print(line)\n",
    "            \n",
    "            time_logged = line.split('UTC')[0][1:].strip()\n",
    "            #time_logged = datetime.strptime(time_logged,'%Y-%m-%d-%H:%M:%S')\n",
    "            #print(time_logged)\n",
    "            \n",
    "            username_message = line.split(']')[1:]\n",
    "            username_message = 'â€”'.join(username_message).strip()\n",
    "            #print(username_message)\n",
    "            try:\n",
    "                temp = username_message.split(\":\")\n",
    "                username = temp[0]\n",
    "                message = temp[1]\n",
    "            except IndexError:\n",
    "                username = temp[0]\n",
    "                message = '-'\n",
    "            #print(username)\n",
    "            #print(message)\n",
    "            \n",
    "            d = {'dt': time_logged,\n",
    "                 'username': username,\n",
    "                 'message': message\n",
    "                    }\n",
    "            data.append(d)\n",
    "            \n",
    "    return pd.DataFrame().from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe224a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is an example\n",
    "chat_df = get_chat_dataframe('[1-21-22] Mongraal - Winning Duo Cash Cup With FaZe BL GUILD Flikk (Webcam Mic) - Chat.txt')\n",
    "\n",
    "chat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef319d89",
   "metadata": {},
   "source": [
    "Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627535d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning\n",
    "import texthero as hero\n",
    "from texthero import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "## customize a pipelifne\n",
    "custom_pipeline = [preprocessing.lowercase, \n",
    "                   preprocessing.remove_punctuation,\n",
    "                   preprocessing.remove_urls]\n",
    "\n",
    "chat_df_temp = hero.clean(chat_df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom STOPWORDS removal -- Need to think about this.Could use the cloud as below \n",
    "from texthero import stopwords\n",
    "default_stopwords = stopwords.DEFAULT\n",
    "custom_stopwords = default_stopwords.union(set(['yo','play',\"twitch\",\"make\", \"use\", \"thank\", \"content\", \"good\", \"use\",\n",
    "                                                \"think\", \"need\", \"harrisheller\", \"like\", \"stream\",\n",
    "                                               \"kekw\",\"catjam\",\"tim\",\"timthetatman\",\"tatkevinh\",\n",
    "                                               \"wipz\",\"docspin\",\"pog\",\"tatlove\",\"lol\",\"lul\",\"omegalul\",\n",
    "                                               \"biblethump\",\"clap\",\"tathypers\",\"pepeja\",\"kappa\",\"tattopd\",\"ppsmoke\",\n",
    "                                               \"pepelaugh\",\"gopackgo\",\"gachihyper\",\"tatkevinh\", \"wipz\",\n",
    "                                               \"pausechamp\",\"yep\",\"lmao\",\"jack\",\"lulw\",\"monkaw\",\"kreygasm\",\n",
    "                                               \"pepega\",\"peped\",\"foxsalt\",\"pogchamp\",\"xqcn\",\"get\",\"back\",\n",
    "                                               \"tattuff\",\"tatfat\",\"tatpumpkin\",\"lmao\",\"sadge\",\"sippy\",\n",
    "                                               \"pogu\",\"poggers\",\"consolecd\",\"widepeepohappy\",\"pogu\",\"tategg2\",\n",
    "                                               \"modcheck\",\"timmy\",\"tathmm\",\"tats\",\"got\",\"com\",\"babyrage\",\n",
    "                                               \"xqcp\",\"tatw\",\"pokiw\",\"know\", \"thats\",\"pepocd\",\"tatafk\",\n",
    "                                               \"4weird\",\"tatkkevin\", \"tatblanket\",\"tatglam\",\"tategg1\",\"wutface\",\n",
    "                                               \"blobdance\", \"kapp\",\"tatbruh\",\"kappapride\",\"facebaby\",\"xqc\",\n",
    "                                               \"xqcm\",\"bora\",\"hyperclap\",\"tatlit\",\"5head\",\"gachibass\", \"go\", \"ur\",\n",
    "                                                \"yes\",\"going\",\"would\",\"im\",\"oh\",\"dez\",\"taty\",\"tk\",\"u\",\"sg\", \"dont\",\n",
    "                                                \"hey\",\"hf\",\"look\",\"anita\",\"anitaheart\",\"leeper\",\"anitahey\",\n",
    "                                                \"anitadab\",\"anitadoubt\",\"anitahands\", \"anitalul\",\"anitapog\",\"anitasword\",\n",
    "                                                \"anitaprime\",\"anitafeels\",\"residentsleeper\",\"anitapride\",\"anital\",\n",
    "                                                \"anitabiscuit\", \"thing\",\"widepeeposad\",\"anitahype\",\"among\", \"lesbianpride\",\n",
    "                                                \"heyguys\",\"seemsgood\",\"notlikethis\",\"hi\",\"also\",\"much\",\"yeah\",\n",
    "                                               ])) ## Add as per requirement\n",
    "# data = hero.remove_stopwords(data, default_stopwords)\n",
    "chat_df_clean = hero.remove_stopwords(chat_df_temp, custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize with wordcloud\n",
    "hero.visualization.wordcloud(data, font_path = None, width = 400, height = 200, max_words=200, \n",
    "                             mask=None, contour_width=0, \n",
    "                             contour_color='PAPAYAWHIP', background_color='WHITE', \n",
    "                             relative_scaling='auto', colormap=None, return_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Tokenize\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(chat_df_clean))\n",
    "\n",
    "print(data_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42723157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adjective, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ae1b2",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a486e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "# !pip install -U pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a876bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-Document Matrix\n",
    "# This converts a collection of text documents to a matrix of token counts. \n",
    "# A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. \n",
    "# In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum number occurences of a word required\n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}')  # num of characters > 3\n",
    "                            \n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905114c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model with Sklearn\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=15,               # Number of topics\n",
    "                                      max_iter=10,                   # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,              # Random state\n",
    "                                      batch_size=128)                # n docs in each learning iter\n",
    "                                            \n",
    "                                      \n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model quality\n",
    "\n",
    "# Log Likelihood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp((-1) * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Best LDA Model\n",
    "# Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \n",
    "# It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator.\n",
    "# This is computationally expensive and usually takes time... \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Search Param\n",
    "params = {'n_components': [5, 10, 15,20], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Grid Search\n",
    "model = GridSearchCV(lda, param_grid=params)\n",
    "\n",
    "# Perform Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31655266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflections:\n",
    "# how important and how frequent certain words are in the corpus - what themes/etc. may emerge. \n",
    "# Show top n keywords for each topic\n",
    "\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
