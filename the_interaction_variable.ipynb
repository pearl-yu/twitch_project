{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLrqAJ9d0dFJOWk/JUydUU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pearl-yu/twitch_project/blob/main/the_interaction_variable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We gotta train a machine learning model to identify streamer action types: If interacting, interaction type. \n",
        "\n",
        "- What variables we want?\n",
        "  - At a session level, interaction levels/proportions to start with. I think it works to just classify each sentence. \n",
        "  - Simple ones: count the number of words. \n",
        "    - Challenge: How to exclude background noise? \n",
        "- We need the labels. \n",
        "  - Just from the videos, classify.\n",
        "  - From the literature? \n",
        "- We need to find a model, fine-tune, then predict. \n",
        "  - What model to choose? Search hugging face.\n",
        "\n",
        "\n",
        "Hence the workflow:\n",
        "- Label a few to get some intuition. \n",
        "  1. From just the transcript, if I try to label 'responding to chat','throwing out questions','talking about content', human accuracy isn't that great I think. \n",
        "    For example, 'You wanna see ...?' It's hard to tell if the streamer is interacting with a viewer, or just throwing out a question if the streamer doesn't say 'Yes, XXX'. \n",
        "    - Either we use some crowdsourcing to label such things. If we want to do this, we still need a set of defined labels, which are actually hard cuz sometimes it's intertwined.\n",
        "    - Or we don't label it to this detail. We want to measure 'interaction efforts', so maybe 'responding to viewers' is just one sub-category. \n",
        "  2. I think one measure could be, not exactly streamer interaction, but engaging efforts. \n",
        "    - This could be simply measured by for example, the number of words/streaming length. For this, we only need the transcription, maybe an ML to distinguish the speaker (pre-trained on hugging face). \n",
        "  3. How to measure 'interaction efforts' more generally? It's easier to label, 'about content' and 'interact'. This is possible. \n",
        "- Literature.\n",
        "- Training dataset and labels. \n",
        "  - Remember to classify background noise. \n",
        "\n",
        "- Find the model on hugging face. \n",
        "- Compile the code.\n",
        "- Model evaluation. "
      ],
      "metadata": {
        "id": "ydGSM7g10wik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEhrPTg00qgN"
      },
      "outputs": [],
      "source": []
    }
  ]
}